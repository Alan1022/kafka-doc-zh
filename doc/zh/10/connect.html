<!--~
  ~ Licensed to the Apache Software Foundation (ASF) under one or more
  ~ contributor license agreements.  See the NOTICE file distributed with
  ~ this work for additional information regarding copyright ownership.
  ~ The ASF licenses this file to You under the Apache License, Version 2.0
  ~ (the "License"); you may not use this file except in compliance with
  ~ the License.  You may obtain a copy of the License at
  ~
  ~    http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  ~-->

<script id="connect-template" type="text/x-handlebars-template">
    <h3><a id="connect_overview" href="#connect_overview">8.1 概述</a></h3>

    <p>Kafka Connect 是一款可扩展并且可靠地在 Apache Kafka 和其他系统之间进行数据传输的工具。 可以很简单的快速定义 <i>connectors</i> 将大量数据从 Kafka 移入和移出. Kafka Connect 可以摄取数据库数据或者收集应用程序的 metrics 存储到 Kafka topics，使得数据可以用于低延迟的流处理。 一个导出的 job 可以将来自 Kafka topic 的数据传输到二级存储，用于系统查询或者批量进行离线分析。</p>

    <p>Kafka Connect 功能包括:</p>
    <ul>
        <li><b>Kafka connectors 通用框架：</b> - Kafka Connect 将其他数据系统和Kafka集成标准化,简化了 connector 的开发,部署和管理</li>
        <li><b>分布式和单机模式</b> - 可以扩展成一个集中式的管理服务，也可以单机方便的开发,测试和生产环境小型的部署。</li>
        <li><b>REST 接口</b> - submit and manage connectors to your Kafka Connect cluster via an easy to use REST API</li>
        <li><b>offset 自动管理</b> - 只需要connectors 的一些信息，Kafka Connect 可以自动管理offset 提交的过程，因此开发人员无需担心开发中offset提交出错的这部分。</li>
        <li><b>分布式的并且可扩展</b> - Kafka Connect 构建在现有的 group 管理协议上。Kafka Connect 集群可以扩展添加更多的workers。</li>
        <li><b>整合流处理/批处理</b> - 利用 Kafka 已有的功能，Kafka Connect 是一个桥接stream 和批处理系统理想的方式。</li>
    </ul>

    <h3><a id="connect_user" href="#connect_user">8.2 用户指南</a></h3>

    <p> quickstart 提供了一个简单的例子，演示如何运行一个单机版的Kafka Connect。 这一节描述如何配置，如何管理Kafka Connect 的更多细节。</p>

    <h4><a id="connect_running" href="#connect_running">运行 Kafka Connect</a></h4>

    <p>Kafka Connect 当前支持两种执行方式: 单机 (单个进程) 和 分布式.</p>

    <p>在单机模式下所有的工作都是在一个进程中运行的。connect的配置项很容易配置和开始使用，当只有一台机器(worker)的时候也是可用的(例如，收集日志文件到kafka)，但是不利于Kafka Connect 的容错。你可以通过下面的命令启动一个单机进程:</p>

    <pre class="brush: bash;">
    &gt; bin/connect-standalone.sh config/connect-standalone.properties connector1.properties [connector2.properties ...]
    </pre>

    <p>第一个参数是 worker 的配置文件. 其中包括 Kafka connection 参数，序列化格式，和如何频繁的提交offsets。 所提供的示例可以在本地良好的运行，使用默认提供的配置 <code>config/server.properties</code> 。它需要调整以配合不同的配置或生产环境部署。所有的workers（独立和分布式）都需要一些配置 :</p>
    <ul>
        <li><code>bootstrap.servers</code> - List of Kafka servers used to bootstrap connections to Kafka</li>
        <li><code>key.converter</code> - Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka. This controls the format of the keys in messages written to or read from Kafka, and since this is independent of connectors it allows any connector to work with any serialization format. Examples of common formats include JSON and Avro.</li>
        <li><code>value.converter</code> - Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka. This controls the format of the values in messages written to or read from Kafka, and since this is independent of connectors it allows any connector to work with any serialization format. Examples of common formats include JSON and Avro.</li>
    </ul>

    <p>单机模式的重要配置如下:</p>
    <ul>
        <li><code>offset.storage.file.filename</code> - 存储 offset 数据的文件</li>
    </ul>

    <p>此处配置的参数适用于由Kafka Connect使用的 producer 和 consumer 访问配置，offset 和 status topic。对于 Kafka source和 sink 任务的配置，可以使用相同的参数，但必须以<code>consumer.</code> 和 <code>producer.</code> 作为前缀。 此外，从 worker 配置中继承的参数只有一个，就是 <code>bootstrap.servers</code>。大多数情况下，这是足够的，因为同一个集群通常用于所有的场景。但是需要注意的是一个安全集群，需要额外的参数才能允许连接。这些参数需要在 worker 配置中设置三次，一次用于管理访问，一次用于 Kafka sinks，还有一次用于 Kafka source。</p>

    <p>其余参数用于 connector 的配置文件，你可以导入尽可能多的配置文件，但是所有的配置文件都将在同一个进程内(在不同的线程上)执行。</p>

    <p>分布式模式下会自动进行负载均衡，允许动态的扩缩容，并提供对 active task，以及这个任务对应的配置和offset提交记录的容错。分布式执行方式和单机模式非常相似:</p>

    <pre class="brush: bash;">
    &gt; bin/connect-distributed.sh config/connect-distributed.properties
    </pre>

    <p>和单机模式不同在于启动的实现类和决定 Kafka connect 进程如何工作的配置参数，如何分配 work,offsets 存储在哪里和任务状态。在分布式模式中，Kafka Connect 存储 offsets,配置和存储在 Kafka topic中的任务状态。建议手动创建Kafka 的 offsets,配置和状态，以实现自己所期望的分区数和备份因子。如果启动Kafka Connect之前没有创建 topic，则会使用默认分区数和复制因子自动创建创建 topic，但这可能不是最适合的。</p>

    <p>特别是，除了上面提到的常用设置之外，以下配置参数在启动集群之前至关重要:</p>
    <ul>
        <li><code>group.id</code> (default <code>connect-cluster</code>) - unique name for the cluster, used in forming the Connect cluster group; note that this <b>must not conflict</b> with consumer group IDs</li>
        <li><code>config.storage.topic</code> (default <code>connect-configs</code>) - topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated, compacted topic. You may need to manually create the topic to ensure the correct configuration as auto created topics may have multiple partitions or be automatically configured for deletion rather than compaction</li>
        <li><code>offset.storage.topic</code> (default <code>connect-offsets</code>) - topic to use for storing offsets; this topic should have many partitions, be replicated, and be configured for compaction</li>
        <li><code>status.storage.topic</code> (default <code>connect-status</code>) - topic to use for storing statuses; this topic can have multiple partitions, and should be replicated and configured for compaction</li>
    </ul>

    <p>注意在分布式模式下 connector 配置不会通过命令行传递。相反，会使用下面提到的 REST API来创建，修改和销毁 connectors。</p>


    <h4><a id="connect_configuring" href="#connect_configuring">Configuring Connectors</a></h4>

    <p>Connector 配置是简单的key-value 映射的格式。对于单机模式，这些配置会在 properties 文件中定义，并通过命令行传递给 Connect 进程。在分布式模式中，它们将被包含在创建（或修改）connector 的请求的JSON格式串中。</p>

    <p>大多数配置都依赖于 connectors,所以在这里不能概述。但是，有几个常见选项可以看一下:</p>

    <ul>
        <li><code>name</code> - Unique name for the connector. Attempting to register again with the same name will fail.</li>
        <li><code>connector.class</code> - The Java class for the connector</li>
        <li><code>tasks.max</code> - The maximum number of tasks that should be created for this connector. The connector may create fewer tasks if it cannot achieve this level of parallelism.</li>
        <li><code>key.converter</code> - (optional) Override the default key converter set by the worker.</li>
        <li><code>value.converter</code> - (optional) Override the default value converter set by the worker.</li>
    </ul>

    <p> <code>connector.class</code> 配置支持多种名称格式：这个 connector class 的全名或者别名。如果 connector 是 org.apache.kafka.connect.file.FileStreamSinkConnector，则可以指定全名，也可以使用FileStreamSink 或 FileStreamSinkConnector 来简化配置。</p>

    <p>Sink connectors 还有一个额外的选项来控制他的输出:</p>
    <ul>
        <li><code>topics</code> - A list of topics to use as input for this connector</li>
    </ul>

    <p>对于任何其他选项，你应该查阅 connector的文档.</p>

    <h4><a id="connect_transforms" href="#connect_transforms">Transformations</a></h4>

    <p>connectors可以配置 transformations 操作，实现轻量级的消息单次修改，他们可以方便地用于数据修改和事件路由。</p>

    <p>A transformation chain 可以在connector 配置中指定。</p>

    <ul>
        <li><code>transforms</code> - List of aliases for the transformation, specifying the order in which the transformations will be applied.</li>
        <li><code>transforms.$alias.type</code> - Fully qualified class name for the transformation.</li>
        <li><code>transforms.$alias.$transformationSpecificConfig</code> Configuration properties for the transformation</li>
    </ul>

    <p>例如，让我们使用内置的 file soucre connector，并使用 transformation 来添加静态字段。</p>

    <p>这个例子中，我们会使用 schemaless json 数据格式。为了使用 schemaless 格式，我们将 <code>connect-standalone.properties</code>  文件中下面两行从true改成false:</p>

    <pre class="brush: text;">
        key.converter.schemas.enable
        value.converter.schemas.enable
    </pre>

    <p>这个 file source connector 读取每行数据作为一个字符串。我们会将每行数据包装进一个 Map 数据结构,然后添加一个二级字段来标识事件的来源。做这样一个操作，我们使用两个 transformations:</p>
    <ul>
        <li><b>HoistField</b> to place the input line inside a Map</li>
        <li><b>InsertField</b> to add the static field. In this example we'll indicate that the record came from a file connector</li>
    </ul>

    <p>添加完 transformations, <code>connect-file-source.properties</code> 文件像下面这样:</p>

    <pre class="brush: text;">
        name=local-file-source
        connector.class=FileStreamSource
        tasks.max=1
        file=test.txt
        topic=connect-test
        transforms=MakeMap, InsertSource
        transforms.MakeMap.type=org.apache.kafka.connect.transforms.HoistField$Value
        transforms.MakeMap.field=line
        transforms.InsertSource.type=org.apache.kafka.connect.transforms.InsertField$Value
        transforms.InsertSource.static.field=data_source
        transforms.InsertSource.static.value=test-file-source
    </pre>

    <p>所有以<code>transforms</code> 为开头的行都将被添加了静态字段用于 transformations 。 你可以看到我们创建的两个 transformations: "InsertSource" 和 "MakeMap" 是我们给的 transformations 的别称. transformation 类型基于下面给的一系列内嵌 transformations。每个 transformation 类型都有额外的配置: HoistField 需要一个配置叫做 "field”,这是 map中原始字符串的字段名称。InsertField transformation 让我们指定字段名称和我们要添加的内容。</p>

    <p>当我们对一个 sample file 运行 file source connector 操作，不做transformations 操作，然后使用<code>kafka-console-consumer.sh</code> 读取数据，结果如下:</p>

    <pre class="brush: text;">
        "foo"
        "bar"
        "hello world"
   </pre>

    <p>然后我们创建一个新的file connector,然后将这个transformations 添加到配置文件中。这次结果如下:</p>

    <pre class="brush: json;">
        {"line":"foo","data_source":"test-file-source"}
        {"line":"bar","data_source":"test-file-source"}
        {"line":"hello world","data_source":"test-file-source"}
    </pre>

    <p>你可以看到我们读取的行现在JSON map的一部分，并且还有一个静态值的额外字段。这只是用 transformations 做的一个简单的例子。.</p>

    <p>Kafka Connect 包含几个广泛适用的数据和 routing transformations </p>

    <ul>
        <li>InsertField - Add a field using either static data or record metadata</li>
        <li>ReplaceField - Filter or rename fields</li>
        <li>MaskField - Replace field with valid null value for the type (0, empty string, etc)</li>
        <li>ValueToKey</li>
        <li>HoistField - Wrap the entire event as a single field inside a Struct or a Map</li>
        <li>ExtractField - Extract a specific field from Struct and Map and include only this field in results</li>
        <li>SetSchemaMetadata - modify the schema name or version</li>
        <li>TimestampRouter - Modify the topic of a record based on original topic and timestamp. Useful when using a sink that needs to write to different tables or indexes based on timestamps</li>
        <li>RegexRouter - modify the topic of a record based on original topic, replacement string and a regular expression</li>
    </ul>

    <p>如何配置每个transformation，参考下面: </p>

    @@include('generated/connect_transforms.html')
    <!--#include virtual="generated/connect_transforms.html" -->

    <h4><a id="connect_rest" href="#connect_rest">REST API</a></h4>

    <p>由于Kafka  Connect 旨在作为服务运行，它还提供了一个用于管理 connectors 的REST API。sssss默认情况下，此服务在端口8083上运行。以下是当前支持的功能:
    </p>

    <ul>
        <li><code>GET /connectors</code> - return a list of active connectors</li>
        <li><code>POST /connectors</code> - create a new connector; the request body should be a JSON object containing a string <code>name</code> field and an object <code>config</code> field with the connector configuration parameters</li>
        <li><code>GET /connectors/{name}</code> - get information about a specific connector</li>
        <li><code>GET /connectors/{name}/config</code> - get the configuration parameters for a specific connector</li>
        <li><code>PUT /connectors/{name}/config</code> - update the configuration parameters for a specific connector</li>
        <li><code>GET /connectors/{name}/status</code> - get current status of the connector, including if it is running, failed, paused, etc., which worker it is assigned to, error information if it has failed, and the state of all its tasks</li>
        <li><code>GET /connectors/{name}/tasks</code> - get a list of tasks currently running for a connector</li>
        <li><code>GET /connectors/{name}/tasks/{taskid}/status</code> - get current status of the task, including if it is running, failed, paused, etc., which worker it is assigned to, and error information if it has failed</li>
        <li><code>PUT /connectors/{name}/pause</code> - pause the connector and its tasks, which stops message processing until the connector is resumed</li>
        <li><code>PUT /connectors/{name}/resume</code> - resume a paused connector (or do nothing if the connector is not paused)</li>
        <li><code>POST /connectors/{name}/restart</code> - restart a connector (typically because it has failed)</li>
        <li><code>POST /connectors/{name}/tasks/{taskId}/restart</code> - restart an individual task (typically because it has failed)</li>
        <li><code>DELETE /connectors/{name}</code> - delete a connector, halting all tasks and deleting its configuration</li>
    </ul>

    <p>Kafka Connect还提供用于获取有关 connector plugin sss信息的REST API:</p>

    <ul>
        <li><code>GET /connector-plugins</code>- return a list of connector plugins installed in the Kafka Connect cluster. Note that the API only checks for connectors on the worker that handles the request, which means you may see inconsistent results, especially during a rolling upgrade if you add new connector jars</li>
        <li><code>PUT /connector-plugins/{connector-type}/config/validate</code> - validate the provided configuration values against the configuration definition. This API performs per config validation, returns suggested values and error messages during validation.</li>
    </ul>

    <h3><a id="connect_development" href="#connect_development">8.3 Connector 开发者指南</a></h3>

    <p>本指南介绍开发人员如何为 Kafka connector 编写新的 connectors，用于Kafka和其他系统之间移动数据。</p>

    <h4><a id="connect_concepts" href="#connect_concepts">Core Concepts 和 APIs</a></h4>

    <h5><a id="connect_connectorsandtasks" href="#connect_connectorsandtasks">Connectors 和 Tasks</a></h5>
    <code>HDFSSinkConnector</code>
    <p>要在Kafka和另一个系统之间复制数据，用户会为想要 pull 数据或者 push 数据的系统创建一个<code>connector</code>。 connector 有两类:<code>SourceConnectors</code> 从其他系统导入数据(e.g.<code>JDBCSourceConnector</code> 会将关系型数据库导入到Kafka中)和<code>SinkConnectors</code>导出数据(e.g. <code>HDFSSinkConnector</code>会将Kafka topic 的内容导出到 HDFS 文件)</p>

    <p><code>Connectors</code> 自身不执行任何数据复制:<code>Connector</code>的配置描述要复制的数据，并且<code>Connector</code> 负责负责将 job 分解为可分发给 worker 的一组 <code>Tasks</code>。这些<code>Tasks</code>也分为两类: <code>SourceTask</code> 和 <code>SinkTask</code>。</p>

    <p>通过分配，每个<code>Task</code> 必须将数据的一部分子集复制到Kafka或者从Kafka复制。在 Kafka Connect中，应该始终可以将这些分配的数据框架化为一组输入和输出流，这些流由具有一致结构的记录组成。有的时候这些映射是显而易见的:一组日志文件中的每个文件都认为是一个流，每条解析的行数据使用相同的模式和偏移量作为字节偏移量存储在文件中。在其他情况下，可能需要花费更多功夫来映射模型: 一个 JDBC connector可以将表映射成stream，但是offset不能确定。可以使用时间戳字段进行增量查询返回新的数据，最后查询的时间戳可以用作偏移量。</p>


    <h5><a id="connect_streamsandrecords" href="#connect_streamsandrecords">Streams 和 Records</a></h5>

    <p>每个stream 都应该是一串 key-value的就。keys和values可以有复杂的数据结构-提供了很多基本类型，arrays，objects和嵌套的数据结构。每个stream 都应该是一串 key-value的就。keys和values可以有复杂的数据结构-提供了很多基本类型，也可以用来表示arrays，objects和嵌套的数据结构。运行时的数据格式不承担任何特定的序列化格式:此转换由框架内部处理。</p>

    <p>除了 key 和 value, records(sources生成的记录和发送到sinks的记录) 关联 stream IDs和offsets。框架会定期的提交已经处理数据的offsets，以便在发生故障时，可以从最后一次提交的offsets恢复处理，避免不必要的重新处理和重复事件。.</p>

    <h5><a id="connect_dynamicconnectors" href="#connect_dynamicconnectors">Dynamic Connectors</a></h5>

    <p>不是所有的jobs都是静态，所以<code>Connector</code> 的实现还要负责监控外部系统是否需要重新更改配置。例如，在<code>JDBCSourceConnector</code>的例子中，这个<code>Connector</code>可能分配一组 tables 给 <code>Task</code>。当一个新的 table 创建了，必须发现这个时间，以便通过更新配置来将新表分配给其中一个<code>Tasks</code>。当发现需要重新配置的变更(或者<code>Tasks</code> 数量)的时候，他会通知框架，并更新相应的<code>Tasks</code>。</p>


    <h4><a id="connect_developing" href="#connect_developing">开发一个简单的 Connector</a></h4>

    <p>开发一个 connector 只需要实现两个接口,  <code>Connector</code> 和 <code>Task</code>接口. 一个简单的例子的源码在Kafka<code>file</code>  package中。 connector 用于单机模式，并拥有 <code>SourceConnector</code> 和<code>SourceTask</code>实现来读取一个文件的每行记录，并将其作为记录发发送，<code>SinkConnector</code>的<code>SinkTask</code>将记录写入到文件。</p>

    <p>本节的其余部分将通过一些代码演示创建 connector 的关键步骤，但开发人员还应参考完整的示例源代码，因为为简洁起见，省略了许多细节。</p>

    <h5><a id="connect_connectorexample" href="#connect_connectorexample">Connector Example</a></h5>

    <p>我们拿 <code>SourceConnector</code> 作为一个简单的例子. <code>SinkConnector</code> 实现是非常相似的. 首先创建一个继承 <code>SourceConnector</code>的类并添加一些字段来存储分析的配置信息(要读取的文件名以及要发送数据的topic):</p>

    <pre class="brush: java;">
    public class FileStreamSourceConnector extends SourceConnector {
        private String filename;
        private String topic;
    </pre>

    <p>要填写的最简单的方法是 <code>taskClass()</code> 方法,这个类应该在 worker中实例化后读取相应的数据:</p>

    <pre class="brush: java;">
    @Override
    public Class&lt;? extends Task&gt; taskClass() {
        return FileStreamSourceTask.class;
    }
    </pre>

    <p>我们会在下面定义 <code>FileStreamSourceTask</code> class 类. 然后,我添加一些标准的生命周期的 methods , <code>start()</code> 和 <code>stop()</code></p>:

    <pre class="brush: java;">
    @Override
    public void start(Map&lt;String, String&gt; props) {
        // The complete version includes error handling as well.
        filename = props.get(FILE_CONFIG);
        topic = props.get(TOPIC_CONFIG);
    }

    @Override
    public void stop() {
        // Nothing to do since no background monitoring is required.
    }
    </pre>

    <p>最后,真正核心的实现在<code>taskConfigs()</code>. 在这个例子里面我们只处理单个文件,因此即使我们可能允许按照
        <code>maxTasks</code> 参数生成更多的任务, 我们也只返回一个 entry 的list:</p>

    <pre class="brush: java;">
    @Override
    public List&lt;Map&lt;String, String&gt;&gt; taskConfigs(int maxTasks) {
        ArrayList&lt;Map&lt;String, String&gt;&gt; configs = new ArrayList&lt;&gt;();
        // Only one input stream makes sense.
        Map&lt;String, String&gt; config = new HashMap&lt;&gt;();
        if (filename != null)
            config.put(FILE_CONFIG, filename);
        config.put(TOPIC_CONFIG, topic);
        configs.add(config);
        return configs;
    }
    </pre>

    <p>尽管没有在示例中使用，但是<code> SourceTask </code>还提供了两个API来提交源系统中的偏移量：<code> commit </code>和<code> commitRecord </code>。这些API是为具有消息确认机制的源系统提供的。覆盖这些方法允许source connector 在源系统中写入Kafka后，批量或单独确认消息。
                <code> commit </code> API将偏移量存储在源系统中，直至由<code> poll </code>返回的偏移量。这个API的实现应该阻塞直到提交完成。在写入Kafka之后，<code> commitRecord </code> API将每个<code> SourceRecord </code>的偏移量保存在 source 系统中。由于Kafka Connect会自动记录偏移量，因此不需要<code> SourceTask </code>来实现它们。在 connector 确实需要确认源系统中的消息的情况下，通常只需要其中一个API。</p>

    <p>即使是多任务, 这个方法实现通常也很简单。它只需要确定输入 task 的数量，这可能需要联系拉取数据的远程服务，然后将数据分解。由于这些用于任务之间分割的方式非常普遍，因为在 <code>ConnectorUtils</code> 工具类中提供了一些实用程序来简化这些方式。</p>

    <p>由于这个简单的例子不包括动态输入。详见在下一节讨论如何触发更新任务配置.</p>

    <h5><a id="connect_taskexample" href="#connect_taskexample">Task Example - Source Task</a></h5>

    <p>Next we'll describe the implementation of the corresponding <code>SourceTask</code>. The implementation is short, but too long to cover completely in this guide. We'll use pseudo-code to describe most of the implementation, but you can refer to the source code for the full example.</p>

    <p>Just as with the connector, we need to create a class inheriting from the appropriate base <code>Task</code> class. It also has some standard lifecycle methods:</p>


    <pre class="brush: java;">
    public class FileStreamSourceTask extends SourceTask {
        String filename;
        InputStream stream;
        String topic;

        @Override
        public void start(Map&lt;String, String&gt; props) {
            filename = props.get(FileStreamSourceConnector.FILE_CONFIG);
            stream = openOrThrowError(filename);
            topic = props.get(FileStreamSourceConnector.TOPIC_CONFIG);
        }

        @Override
        public synchronized void stop() {
            stream.close();
        }
    </pre>

    <p>These are slightly simplified versions, but show that that these methods should be relatively simple and the only work they should perform is allocating or freeing resources. There are two points to note about this implementation. First, the <code>start()</code> method does not yet handle resuming from a previous offset, which will be addressed in a later section. Second, the <code>stop()</code> method is synchronized. This will be necessary because <code>SourceTasks</code> are given a dedicated thread which they can block indefinitely, so they need to be stopped with a call from a different thread in the Worker.</p>

    <p>Next, we implement the main functionality of the task, the <code>poll()</code> method which gets events from the input system and returns a <code>List&lt;SourceRecord&gt;</code>:</p>

    <pre class="brush: java;">
    @Override
    public List&lt;SourceRecord&gt; poll() throws InterruptedException {
        try {
            ArrayList&lt;SourceRecord&gt; records = new ArrayList&lt;&gt;();
            while (streamValid(stream) &amp;&amp; records.isEmpty()) {
                LineAndOffset line = readToNextLine(stream);
                if (line != null) {
                    Map&lt;String, Object&gt; sourcePartition = Collections.singletonMap("filename", filename);
                    Map&lt;String, Object&gt; sourceOffset = Collections.singletonMap("position", streamOffset);
                    records.add(new SourceRecord(sourcePartition, sourceOffset, topic, Schema.STRING_SCHEMA, line));
                } else {
                    Thread.sleep(1);
                }
            }
            return records;
        } catch (IOException e) {
            // Underlying stream was killed, probably as a result of calling stop. Allow to return
            // null, and driving thread will handle any shutdown if necessary.
        }
        return null;
    }
    </pre>

    <p>Again, we've omitted some details, but we can see the important steps: the <code>poll()</code> method is going to be called repeatedly, and for each call it will loop trying to read records from the file. For each line it reads, it also tracks the file offset. It uses this information to create an output <code>SourceRecord</code> with four pieces of information: the source partition (there is only one, the single file being read), source offset (byte offset in the file), output topic name, and output value (the line, and we include a schema indicating this value will always be a string). Other variants of the <code>SourceRecord</code> constructor can also include a specific output partition and a key.</p>

    <p>Note that this implementation uses the normal Java <code>InputStream</code> interface and may sleep if data is not available. This is acceptable because Kafka Connect provides each task with a dedicated thread. While task implementations have to conform to the basic <code>poll()</code> interface, they have a lot of flexibility in how they are implemented. In this case, an NIO-based implementation would be more efficient, but this simple approach works, is quick to implement, and is compatible with older versions of Java.</p>

    <h5><a id="connect_sinktasks" href="#connect_sinktasks">Sink Tasks</a></h5>

    <p>The previous section described how to implement a simple <code>SourceTask</code>. Unlike <code>SourceConnector</code> and <code>SinkConnector</code>, <code>SourceTask</code> and <code>SinkTask</code> have very different interfaces because <code>SourceTask</code> uses a pull interface and <code>SinkTask</code> uses a push interface. Both share the common lifecycle methods, but the <code>SinkTask</code> interface is quite different:</p>

    <pre class="brush: java;">
    public abstract class SinkTask implements Task {
        public void initialize(SinkTaskContext context) {
            this.context = context;
        }

        public abstract void put(Collection&lt;SinkRecord&gt; records);

        public void flush(Map&lt;TopicPartition, OffsetAndMetadata&gt; currentOffsets) {
        }
    </pre>

    <p>The <code>SinkTask</code> documentation contains full details, but this interface is nearly as simple as the <code>SourceTask</code>. The <code>put()</code> method should contain most of the implementation, accepting sets of <code>SinkRecords</code>, performing any required translation, and storing them in the destination system. This method does not need to ensure the data has been fully written to the destination system before returning. In fact, in many cases internal buffering will be useful so an entire batch of records can be sent at once, reducing the overhead of inserting events into the downstream data store. The <code>SinkRecords</code> contain essentially the same information as <code>SourceRecords</code>: Kafka topic, partition, offset and the event key and value.</p>

    <p>The <code>flush()</code> method is used during the offset commit process, which allows tasks to recover from failures and resume from a safe point such that no events will be missed. The method should push any outstanding data to the destination system and then block until the write has been acknowledged. The <code>offsets</code> parameter can often be ignored, but is useful in some cases where implementations want to store offset information in the destination store to provide exactly-once
        delivery. For example, an HDFS connector could do this and use atomic move operations to make sure the <code>flush()</code> operation atomically commits the data and offsets to a final location in HDFS.</p>


    <h5><a id="connect_resuming" href="#connect_resuming">Resuming from Previous Offsets</a></h5>

    <p>The <code>SourceTask</code> implementation included a stream ID (the input filename) and offset (position in the file) with each record. The framework uses this to commit offsets periodically so that in the case of a failure, the task can recover and minimize the number of events that are reprocessed and possibly duplicated (or to resume from the most recent offset if Kafka Connect was stopped gracefully, e.g. in standalone mode or due to a job reconfiguration). This commit process is completely automated by the framework, but only the connector knows how to seek back to the right position in the input stream to resume from that location.</p>

    <p>To correctly resume upon startup, the task can use the <code>SourceContext</code> passed into its <code>initialize()</code> method to access the offset data. In <code>initialize()</code>, we would add a bit more code to read the offset (if it exists) and seek to that position:</p>

    <pre class="brush: java;">
        stream = new FileInputStream(filename);
        Map&lt;String, Object&gt; offset = context.offsetStorageReader().offset(Collections.singletonMap(FILENAME_FIELD, filename));
        if (offset != null) {
            Long lastRecordedOffset = (Long) offset.get("position");
            if (lastRecordedOffset != null)
                seekToOffset(stream, lastRecordedOffset);
        }
    </pre>

    <p>Of course, you might need to read many keys for each of the input streams. The <code>OffsetStorageReader</code> interface also allows you to issue bulk reads to efficiently load all offsets, then apply them by seeking each input stream to the appropriate position.</p>

    <h4><a id="connect_dynamicio" href="#connect_dynamicio">Dynamic Input/Output Streams</a></h4>

    <p>Kafka Connect is intended to define bulk data copying jobs, such as copying an entire database rather than creating many jobs to copy each table individually. One consequence of this design is that the set of input or output streams for a connector can vary over time.</p>

    <p>Source connectors need to monitor the source system for changes, e.g. table additions/deletions in a database. When they pick up changes, they should notify the framework via the <code>ConnectorContext</code> object that reconfiguration is necessary. For example, in a <code>SourceConnector</code>:</p>

    <pre class="brush: java;">
        if (inputsChanged())
            this.context.requestTaskReconfiguration();
    </pre>

    <p>The framework will promptly request new configuration information and update the tasks, allowing them to gracefully commit their progress before reconfiguring them. Note that in the <code>SourceConnector</code> this monitoring is currently left up to the connector implementation. If an extra thread is required to perform this monitoring, the connector must allocate it itself.</p>

    <p>Ideally this code for monitoring changes would be isolated to the <code>Connector</code> and tasks would not need to worry about them. However, changes can also affect tasks, most commonly when one of their input streams is destroyed in the input system, e.g. if a table is dropped from a database. If the <code>Task</code> encounters the issue before the <code>Connector</code>, which will be common if the <code>Connector</code> needs to poll for changes, the <code>Task</code> will need to handle the subsequent error. Thankfully, this can usually be handled simply by catching and handling the appropriate exception.</p>

    <p><code>SinkConnectors</code> usually only have to handle the addition of streams, which may translate to new entries in their outputs (e.g., a new database table). The framework manages any changes to the Kafka input, such as when the set of input topics changes because of a regex subscription. <code>SinkTasks</code> should expect new input streams, which may require creating new resources in the downstream system, such as a new table in a database. The trickiest situation to handle in these cases may be conflicts between multiple <code>SinkTasks</code> seeing a new input stream for the first time and simultaneously trying to create the new resource. <code>SinkConnectors</code>, on the other hand, will generally require no special code for handling a dynamic set of streams.</p>

    <h4><a id="connect_configs" href="#connect_configs">Connect Configuration Validation</a></h4>

    <p>Kafka Connect allows you to validate connector configurations before submitting a connector to be executed and can provide feedback about errors and recommended values. To take advantage of this, connector developers need to provide an implementation of <code>config()</code> to expose the configuration definition to the framework.</p>

    <p>The following code in <code>FileStreamSourceConnector</code> defines the configuration and exposes it to the framework.</p>

    <pre class="brush: java;">
        private static final ConfigDef CONFIG_DEF = new ConfigDef()
            .define(FILE_CONFIG, Type.STRING, Importance.HIGH, "Source filename.")
            .define(TOPIC_CONFIG, Type.STRING, Importance.HIGH, "The topic to publish data to");

        public ConfigDef config() {
            return CONFIG_DEF;
        }
    </pre>

    <p><code>ConfigDef</code> class is used for specifying the set of expected configurations. For each configuration, you can specify the name, the type, the default value, the documentation, the group information, the order in the group, the width of the configuration value and the name suitable for display in the UI. Plus, you can provide special validation logic used for single configuration validation by overriding the <code>Validator</code> class. Moreover, as there may be dependencies between configurations, for example, the valid values and visibility of a configuration may change according to the values of other configurations. To handle this, <code>ConfigDef</code> allows you to specify the dependents of a configuration and to provide an implementation of <code>Recommender</code> to get valid values and set visibility of a configuration given the current configuration values.</p>

    <p>Also, the <code>validate()</code> method in <code>Connector</code> provides a default validation implementation which returns a list of allowed configurations together with configuration errors and recommended values for each configuration. However, it does not use the recommended values for configuration validation. You may provide an override of the default implementation for customized configuration validation, which may use the recommended values.</p>

    <h4><a id="connect_schemas" href="#connect_schemas">Working with Schemas</a></h4>

    <p>The FileStream connectors are good examples because they are simple, but they also have trivially structured data -- each line is just a string. Almost all practical connectors will need schemas with more complex data formats.</p>

    <p>To create more complex data, you'll need to work with the Kafka Connect <code>data</code> API. Most structured records will need to interact with two classes in addition to primitive types: <code>Schema</code> and <code>Struct</code>.</p>

    <p>The API documentation provides a complete reference, but here is a simple example creating a <code>Schema</code> and <code>Struct</code>:</p>

    <pre class="brush: java;">
    Schema schema = SchemaBuilder.struct().name(NAME)
        .field("name", Schema.STRING_SCHEMA)
        .field("age", Schema.INT_SCHEMA)
        .field("admin", new SchemaBuilder.boolean().defaultValue(false).build())
        .build();

    Struct struct = new Struct(schema)
        .put("name", "Barbara Liskov")
        .put("age", 75);
    </pre>

    <p>If you are implementing a source connector, you'll need to decide when and how to create schemas. Where possible, you should avoid recomputing them as much as possible. For example, if your connector is guaranteed to have a fixed schema, create it statically and reuse a single instance.</p>

    <p>However, many connectors will have dynamic schemas. One simple example of this is a database connector. Considering even just a single table, the schema will not be predefined for the entire connector (as it varies from table to table). But it also may not be fixed for a single table over the lifetime of the connector since the user may execute an <code>ALTER TABLE</code> command. The connector must be able to detect these changes and react appropriately.</p>

    <p>Sink connectors are usually simpler because they are consuming data and therefore do not need to create schemas. However, they should take just as much care to validate that the schemas they receive have the expected format. When the schema does not match -- usually indicating the upstream producer is generating invalid data that cannot be correctly translated to the destination system -- sink connectors should throw an exception to indicate this error to the system.</p>

    <h4><a id="connect_administration" href="#connect_administration">Kafka Connect Administration</a></h4>

    <p>
        Kafka Connect's <a href="#connect_rest">REST layer</a> provides a set of APIs to enable administration of the cluster. This includes APIs to view the configuration of connectors and the status of their tasks, as well as to alter their current behavior (e.g. changing configuration and restarting tasks).
    </p>

    <p>
        When a connector is first submitted to the cluster, the workers rebalance the full set of connectors in the cluster and their tasks so that each worker has approximately the same amount of work. This same rebalancing procedure is also used when connectors increase or decrease the number of tasks they require, or when a connector's configuration is changed. You can use the REST API to view the current status of a connector and its tasks, including the id of the worker to which each was assigned. For example, querying the status of a file source (using <code>GET /connectors/file-source/status</code>) might produce output like the following:
    </p>

    <pre class="brush: json;">
    {
    "name": "file-source",
    "connector": {
        "state": "RUNNING",
        "worker_id": "192.168.1.208:8083"
    },
    "tasks": [
        {
        "id": 0,
        "state": "RUNNING",
        "worker_id": "192.168.1.209:8083"
        }
    ]
    }
    </pre>

    <p>
        Connectors and their tasks publish status updates to a shared topic (configured with <code>status.storage.topic</code>) which all workers in the cluster monitor. Because the workers consume this topic asynchronously, there is typically a (short) delay before a state change is visible through the status API. The following states are possible for a connector or one of its tasks:
    </p>

    <ul>
        <li><b>UNASSIGNED:</b> The connector/task has not yet been assigned to a worker.</li>
        <li><b>RUNNING:</b> The connector/task is running.</li>
        <li><b>PAUSED:</b> The connector/task has been administratively paused.</li>
        <li><b>FAILED:</b> The connector/task has failed (usually by raising an exception, which is reported in the status output).</li>
    </ul>

    <p>
        In most cases, connector and task states will match, though they may be different for short periods of time when changes are occurring or if tasks have failed. For example, when a connector is first started, there may be a noticeable delay before the connector and its tasks have all transitioned to the RUNNING state. States will also diverge when tasks fail since Connect does not automatically restart failed tasks. To restart a connector/task manually, you can use the restart APIs listed above. Note that if you try to restart a task while a rebalance is taking place, Connect will return a 409 (Conflict) status code. You can retry after the rebalance completes, but it might not be necessary since rebalances effectively restart all the connectors and tasks in the cluster.
    </p>

    <p>
        It's sometimes useful to temporarily stop the message processing of a connector. For example, if the remote system is undergoing maintenance, it would be preferable for source connectors to stop polling it for new data instead of filling logs with exception spam. For this use case, Connect offers a pause/resume API. While a source connector is paused, Connect will stop polling it for additional records. While a sink connector is paused, Connect will stop pushing new messages to it. The pause state is persistent, so even if you restart the cluster, the connector will not begin message processing again until the task has been resumed. Note that there may be a delay before all of a connector's tasks have transitioned to the PAUSED state since it may take time for them to finish whatever processing they were in the middle of when being paused. Additionally, failed tasks will not transition to the PAUSED state until they have been restarted.
    </p>
</script>

<div class="p-connect"></div>
