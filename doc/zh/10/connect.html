<!--~
  ~ Licensed to the Apache Software Foundation (ASF) under one or more
  ~ contributor license agreements.  See the NOTICE file distributed with
  ~ this work for additional information regarding copyright ownership.
  ~ The ASF licenses this file to You under the Apache License, Version 2.0
  ~ (the "License"); you may not use this file except in compliance with
  ~ the License.  You may obtain a copy of the License at
  ~
  ~    http://www.apache.org/licenses/LICENSE-2.0
  ~
  ~ Unless required by applicable law or agreed to in writing, software
  ~ distributed under the License is distributed on an "AS IS" BASIS,
  ~ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  ~ See the License for the specific language governing permissions and
  ~ limitations under the License.
  ~-->

<script id="connect-template" type="text/x-handlebars-template">
    <h3><a id="connect_overview" href="#connect_overview">8.1 概述</a></h3>

    <p>Kafka Connect 是一款可扩展并且可靠地在 Apache Kafka 和其他系统之间进行数据传输的工具。 可以很简单的快速定义 <i>connectors</i> 将大量数据从 Kafka 移入和移出. Kafka Connect 可以摄取数据库数据或者收集应用程序的 metrics 存储到 Kafka topics，使得数据可以用于低延迟的流处理。 一个导出的 job 可以将来自 Kafka topic 的数据传输到二级存储，用于系统查询或者批量进行离线分析。</p>

    <p>Kafka Connect 功能包括:</p>
    <ul>
        <li><b>Kafka connectors 通用框架：</b> - Kafka Connect 将其他数据系统和Kafka集成标准化,简化了 connector 的开发,部署和管理</li>
        <li><b>分布式和单机模式</b> - 可以扩展成一个集中式的管理服务，也可以单机方便的开发,测试和生产环境小型的部署。</li>
        <li><b>REST 接口</b> - submit and manage connectors to your Kafka Connect cluster via an easy to use REST API</li>
        <li><b>offset 自动管理</b> - 只需要connectors 的一些信息，Kafka Connect 可以自动管理offset 提交的过程，因此开发人员无需担心开发中offset提交出错的这部分。</li>
        <li><b>分布式的并且可扩展</b> - Kafka Connect 构建在现有的 group 管理协议上。Kafka Connect 集群可以扩展添加更多的workers。</li>
        <li><b>整合流处理/批处理</b> - 利用 Kafka 已有的功能，Kafka Connect 是一个桥接stream 和批处理系统理想的方式。</li>
    </ul>

    <h3><a id="connect_user" href="#connect_user">8.2 用户指南</a></h3>

    <p> quickstart 提供了一个简单的例子，演示如何运行一个单机版的Kafka Connect。 这一节描述如何配置，如何管理Kafka Connect 的更多细节。</p>

    <h4><a id="connect_running" href="#connect_running">运行 Kafka Connect</a></h4>

    <p>Kafka Connect 当前支持两种执行方式: 单机 (单个进程) 和 分布式.</p>

    <p>在单机模式下所有的工作都是在一个进程中运行的。connect的配置项很容易配置和开始使用，当只有一台机器(worker)的时候也是可用的(例如，收集日志文件到kafka)，但是不利于Kafka Connect 的容错。你可以通过下面的命令启动一个单机进程:</p>

    <pre class="brush: bash;">
    &gt; bin/connect-standalone.sh config/connect-standalone.properties connector1.properties [connector2.properties ...]
    </pre>

    <p>第一个参数是 worker 的配置文件. 其中包括 Kafka connection 参数，序列化格式，和如何频繁的提交offsets。 所提供的示例可以在本地良好的运行，使用默认提供的配置 <code>config/server.properties</code> 。它需要调整以配合不同的配置或生产环境部署。所有的workers（独立和分布式）都需要一些配置 :</p>
    <ul>
        <li><code>bootstrap.servers</code> - List of Kafka servers used to bootstrap connections to Kafka</li>
        <li><code>key.converter</code> - Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka. This controls the format of the keys in messages written to or read from Kafka, and since this is independent of connectors it allows any connector to work with any serialization format. Examples of common formats include JSON and Avro.</li>
        <li><code>value.converter</code> - Converter class used to convert between Kafka Connect format and the serialized form that is written to Kafka. This controls the format of the values in messages written to or read from Kafka, and since this is independent of connectors it allows any connector to work with any serialization format. Examples of common formats include JSON and Avro.</li>
    </ul>

    <p>单机模式的重要配置如下:</p>
    <ul>
        <li><code>offset.storage.file.filename</code> - 存储 offset 数据的文件</li>
    </ul>

    <p>此处配置的参数适用于由Kafka Connect使用的 producer 和 consumer 访问配置，offset 和 status topic。对于 Kafka source和 sink 任务的配置，可以使用相同的参数，但必须以<code>consumer.</code> 和 <code>producer.</code> 作为前缀。 此外，从 worker 配置中继承的参数只有一个，就是 <code>bootstrap.servers</code>。大多数情况下，这是足够的，因为同一个集群通常用于所有的场景。但是需要注意的是一个安全集群，需要额外的参数才能允许连接。这些参数需要在 worker 配置中设置三次，一次用于管理访问，一次用于 Kafka sinks，还有一次用于 Kafka source。</p>

    <p>其余参数用于 connector 的配置文件，你可以导入尽可能多的配置文件，但是所有的配置文件都将在同一个进程内(在不同的线程上)执行。</p>

    <p>分布式模式下会自动进行负载均衡，允许动态的扩缩容，并提供对 active task，以及这个任务对应的配置和offset提交记录的容错。分布式执行方式和单机模式非常相似:</p>

    <pre class="brush: bash;">
    &gt; bin/connect-distributed.sh config/connect-distributed.properties
    </pre>

    <p>和单机模式不同在于启动的实现类和决定 Kafka connect 进程如何工作的配置参数，如何分配 work,offsets 存储在哪里和任务状态。在分布式模式中，Kafka Connect 存储 offsets,配置和存储在 Kafka topic中的任务状态。建议手动创建Kafka 的 offsets,配置和状态，以实现自己所期望的分区数和备份因子。如果启动Kafka Connect之前没有创建 topic，则会使用默认分区数和复制因子自动创建创建 topic，但这可能不是最适合的。</p>

    <p>特别是，除了上面提到的常用设置之外，以下配置参数在启动集群之前至关重要:</p>
    <ul>
        <li><code>group.id</code> (default <code>connect-cluster</code>) - unique name for the cluster, used in forming the Connect cluster group; note that this <b>must not conflict</b> with consumer group IDs</li>
        <li><code>config.storage.topic</code> (default <code>connect-configs</code>) - topic to use for storing connector and task configurations; note that this should be a single partition, highly replicated, compacted topic. You may need to manually create the topic to ensure the correct configuration as auto created topics may have multiple partitions or be automatically configured for deletion rather than compaction</li>
        <li><code>offset.storage.topic</code> (default <code>connect-offsets</code>) - topic to use for storing offsets; this topic should have many partitions, be replicated, and be configured for compaction</li>
        <li><code>status.storage.topic</code> (default <code>connect-status</code>) - topic to use for storing statuses; this topic can have multiple partitions, and should be replicated and configured for compaction</li>
    </ul>

    <p>注意在分布式模式下 connector 配置不会通过命令行传递。相反，会使用下面提到的 REST API来创建，修改和销毁 connectors。</p>


    <h4><a id="connect_configuring" href="#connect_configuring">Configuring Connectors</a></h4>

    <p>Connector 配置是简单的key-value 映射的格式。对于单机模式，这些配置会在 properties 文件中定义，并通过命令行传递给 Connect 进程。在分布式模式中，它们将被包含在创建（或修改）connector 的请求的JSON格式串中。</p>

    <p>大多数配置都依赖于 connectors,所以在这里不能概述。但是，有几个常见选项可以看一下:</p>

    <ul>
        <li><code>name</code> - Unique name for the connector. Attempting to register again with the same name will fail.</li>
        <li><code>connector.class</code> - The Java class for the connector</li>
        <li><code>tasks.max</code> - The maximum number of tasks that should be created for this connector. The connector may create fewer tasks if it cannot achieve this level of parallelism.</li>
        <li><code>key.converter</code> - (optional) Override the default key converter set by the worker.</li>
        <li><code>value.converter</code> - (optional) Override the default value converter set by the worker.</li>
    </ul>

    <p> <code>connector.class</code> 配置支持多种名称格式：这个 connector class 的全名或者别名。如果 connector 是 org.apache.kafka.connect.file.FileStreamSinkConnector，则可以指定全名，也可以使用FileStreamSink 或 FileStreamSinkConnector 来简化配置。</p>

    <p>Sink connectors 还有一个额外的选项来控制他的输出:</p>
    <ul>
        <li><code>topics</code> - A list of topics to use as input for this connector</li>
    </ul>

    <p>对于任何其他选项，你应该查阅 connector的文档.</p>

    <h4><a id="connect_transforms" href="#connect_transforms">Transformations</a></h4>

    <p>connectors可以配置 transformations 操作，实现轻量级的消息单次修改，他们可以方便地用于数据修改和事件路由。</p>

    <p>A transformation chain 可以在connector 配置中指定。</p>

    <ul>
        <li><code>transforms</code> - List of aliases for the transformation, specifying the order in which the transformations will be applied.</li>
        <li><code>transforms.$alias.type</code> - Fully qualified class name for the transformation.</li>
        <li><code>transforms.$alias.$transformationSpecificConfig</code> Configuration properties for the transformation</li>
    </ul>

    <p>例如，让我们使用内置的 file soucre connector，并使用 transformation 来添加静态字段。</p>

    <p>这个例子中，我们会使用 schemaless json 数据格式。为了使用 schemaless 格式，我们将 <code>connect-standalone.properties</code>  文件中下面两行从true改成false:</p>

    <pre class="brush: text;">
        key.converter.schemas.enable
        value.converter.schemas.enable
    </pre>

    <p>这个 file source connector 读取每行数据作为一个字符串。我们会将每行数据包装进一个 Map 数据结构,然后添加一个二级字段来标识事件的来源。做这样一个操作，我们使用两个 transformations:</p>
    <ul>
        <li><b>HoistField</b> to place the input line inside a Map</li>
        <li><b>InsertField</b> to add the static field. In this example we'll indicate that the record came from a file connector</li>
    </ul>

    <p>添加完 transformations, <code>connect-file-source.properties</code> 文件像下面这样:</p>

    <pre class="brush: text;">
        name=local-file-source
        connector.class=FileStreamSource
        tasks.max=1
        file=test.txt
        topic=connect-test
        transforms=MakeMap, InsertSource
        transforms.MakeMap.type=org.apache.kafka.connect.transforms.HoistField$Value
        transforms.MakeMap.field=line
        transforms.InsertSource.type=org.apache.kafka.connect.transforms.InsertField$Value
        transforms.InsertSource.static.field=data_source
        transforms.InsertSource.static.value=test-file-source
    </pre>

    <p>所有以<code>transforms</code> 为开头的行都将被添加了静态字段用于 transformations 。 你可以看到我们创建的两个 transformations: "InsertSource" 和 "MakeMap" 是我们给的 transformations 的别称. transformation 类型基于下面给的一系列内嵌 transformations。每个 transformation 类型都有额外的配置: HoistField 需要一个配置叫做 "field”,这是 map中原始字符串的字段名称。InsertField transformation 让我们指定字段名称和我们要添加的内容。</p>

    <p>当我们对一个 sample file 运行 file source connector 操作，不做transformations 操作，然后使用<code>kafka-console-consumer.sh</code> 读取数据，结果如下:</p>

    <pre class="brush: text;">
        "foo"
        "bar"
        "hello world"
   </pre>

    <p>然后我们创建一个新的file connector,然后将这个transformations 添加到配置文件中。这次结果如下:</p>

    <pre class="brush: json;">
        {"line":"foo","data_source":"test-file-source"}
        {"line":"bar","data_source":"test-file-source"}
        {"line":"hello world","data_source":"test-file-source"}
    </pre>

    <p>你可以看到我们读取的行现在JSON map的一部分，并且还有一个静态值的额外字段。这只是用 transformations 做的一个简单的例子。.</p>

    <p>Kafka Connect 包含几个广泛适用的数据和 routing transformations </p>

    <ul>
        <li>InsertField - Add a field using either static data or record metadata</li>
        <li>ReplaceField - Filter or rename fields</li>
        <li>MaskField - Replace field with valid null value for the type (0, empty string, etc)</li>
        <li>ValueToKey</li>
        <li>HoistField - Wrap the entire event as a single field inside a Struct or a Map</li>
        <li>ExtractField - Extract a specific field from Struct and Map and include only this field in results</li>
        <li>SetSchemaMetadata - modify the schema name or version</li>
        <li>TimestampRouter - Modify the topic of a record based on original topic and timestamp. Useful when using a sink that needs to write to different tables or indexes based on timestamps</li>
        <li>RegexRouter - modify the topic of a record based on original topic, replacement string and a regular expression</li>
    </ul>

    <p>如何配置每个transformation，参考下面: </p>

    @@include('generated/connect_transforms.html')
    <!--#include virtual="generated/connect_transforms.html" -->

    <h4><a id="connect_rest" href="#connect_rest">REST API</a></h4>

    <p>由于Kafka  Connect 旨在作为服务运行，它还提供了一个用于管理 connectors 的REST API。sssss默认情况下，此服务在端口8083上运行。以下是当前支持的功能:
    </p>

    <ul>
        <li><code>GET /connectors</code> - return a list of active connectors</li>
        <li><code>POST /connectors</code> - create a new connector; the request body should be a JSON object containing a string <code>name</code> field and an object <code>config</code> field with the connector configuration parameters</li>
        <li><code>GET /connectors/{name}</code> - get information about a specific connector</li>
        <li><code>GET /connectors/{name}/config</code> - get the configuration parameters for a specific connector</li>
        <li><code>PUT /connectors/{name}/config</code> - update the configuration parameters for a specific connector</li>
        <li><code>GET /connectors/{name}/status</code> - get current status of the connector, including if it is running, failed, paused, etc., which worker it is assigned to, error information if it has failed, and the state of all its tasks</li>
        <li><code>GET /connectors/{name}/tasks</code> - get a list of tasks currently running for a connector</li>
        <li><code>GET /connectors/{name}/tasks/{taskid}/status</code> - get current status of the task, including if it is running, failed, paused, etc., which worker it is assigned to, and error information if it has failed</li>
        <li><code>PUT /connectors/{name}/pause</code> - pause the connector and its tasks, which stops message processing until the connector is resumed</li>
        <li><code>PUT /connectors/{name}/resume</code> - resume a paused connector (or do nothing if the connector is not paused)</li>
        <li><code>POST /connectors/{name}/restart</code> - restart a connector (typically because it has failed)</li>
        <li><code>POST /connectors/{name}/tasks/{taskId}/restart</code> - restart an individual task (typically because it has failed)</li>
        <li><code>DELETE /connectors/{name}</code> - delete a connector, halting all tasks and deleting its configuration</li>
    </ul>

    <p>Kafka Connect还提供用于获取有关 connector plugin sss信息的REST API:</p>

    <ul>
        <li><code>GET /connector-plugins</code>- return a list of connector plugins installed in the Kafka Connect cluster. Note that the API only checks for connectors on the worker that handles the request, which means you may see inconsistent results, especially during a rolling upgrade if you add new connector jars</li>
        <li><code>PUT /connector-plugins/{connector-type}/config/validate</code> - validate the provided configuration values against the configuration definition. This API performs per config validation, returns suggested values and error messages during validation.</li>
    </ul>

    <h3><a id="connect_development" href="#connect_development">8.3 Connector 开发者指南</a></h3>

    <p>本指南介绍开发人员如何为 Kafka connector 编写新的 connectors，用于Kafka和其他系统之间移动数据。</p>

    <h4><a id="connect_concepts" href="#connect_concepts">Core Concepts 和 APIs</a></h4>

    <h5><a id="connect_connectorsandtasks" href="#connect_connectorsandtasks">Connectors 和 Tasks</a></h5>
    <code>HDFSSinkConnector</code>
    <p>要在Kafka和另一个系统之间复制数据，用户会为想要 pull 数据或者 push 数据的系统创建一个<code>connector</code>。 connector 有两类:<code>SourceConnectors</code> 从其他系统导入数据(e.g.<code>JDBCSourceConnector</code> 会将关系型数据库导入到Kafka中)和<code>SinkConnectors</code>导出数据(e.g. <code>HDFSSinkConnector</code>会将Kafka topic 的内容导出到 HDFS 文件)</p>

    <p><code>Connectors</code> 自身不执行任何数据复制:<code>Connector</code>的配置描述要复制的数据，并且<code>Connector</code> 负责负责将 job 分解为可分发给 worker 的一组 <code>Tasks</code>。这些<code>Tasks</code>也分为两类: <code>SourceTask</code> 和 <code>SinkTask</code>。</p>

    <p>通过分配，每个<code>Task</code> 必须将数据的一部分子集复制到Kafka或者从Kafka复制。在 Kafka Connect中，应该始终可以将这些分配的数据框架化为一组输入和输出流，这些流由具有一致结构的记录组成。有的时候这些映射是显而易见的:一组日志文件中的每个文件都认为是一个流，每条解析的行数据使用相同的模式和偏移量作为字节偏移量存储在文件中。在其他情况下，可能需要花费更多功夫来映射模型: 一个 JDBC connector可以将表映射成stream，但是offset不能确定。可以使用时间戳字段进行增量查询返回新的数据，最后查询的时间戳可以用作偏移量。</p>


    <h5><a id="connect_streamsandrecords" href="#connect_streamsandrecords">Streams 和 Records</a></h5>

    <p>每个stream 都应该是一串 key-value的就。keys和values可以有复杂的数据结构-提供了很多基本类型，arrays，objects和嵌套的数据结构。每个stream 都应该是一串 key-value的就。keys和values可以有复杂的数据结构-提供了很多基本类型，也可以用来表示arrays，objects和嵌套的数据结构。运行时的数据格式不承担任何特定的序列化格式:此转换由框架内部处理。</p>

    <p>除了 key 和 value, records(sources生成的记录和发送到sinks的记录) 关联 stream IDs和offsets。框架会定期的提交已经处理数据的offsets，以便在发生故障时，可以从最后一次提交的offsets恢复处理，避免不必要的重新处理和重复事件。.</p>

    <h5><a id="connect_dynamicconnectors" href="#connect_dynamicconnectors">Dynamic Connectors</a></h5>

    <p>不是所有的jobs都是静态，所以<code>Connector</code> 的实现还要负责监控外部系统是否需要重新更改配置。例如，在<code>JDBCSourceConnector</code>的例子中，这个<code>Connector</code>可能分配一组 tables 给 <code>Task</code>。当一个新的 table 创建了，必须发现这个时间，以便通过更新配置来将新表分配给其中一个<code>Tasks</code>。当发现需要重新配置的变更(或者<code>Tasks</code> 数量)的时候，他会通知框架，并更新相应的<code>Tasks</code>。</p>


    <h4><a id="connect_developing" href="#connect_developing">开发一个简单的 Connector</a></h4>

    <p>开发一个 connector 只需要实现两个接口,  <code>Connector</code> 和 <code>Task</code>接口. 一个简单的例子的源码在Kafka<code>file</code>  package中。 connector 用于单机模式，并拥有 <code>SourceConnector</code> 和<code>SourceTask</code>实现来读取一个文件的每行记录，并将其作为记录发发送，<code>SinkConnector</code>的<code>SinkTask</code>将记录写入到文件。</p>

    <p>本节的其余部分将通过一些代码演示创建 connector 的关键步骤，但开发人员还应参考完整的示例源代码，因为为简洁起见，省略了许多细节。</p>

    <h5><a id="connect_connectorexample" href="#connect_connectorexample">Connector Example</a></h5>

    <p>We'll cover the <code>SourceConnector</code> as a simple example. <code>SinkConnector</code> implementations are very similar. Start by creating the class that inherits from <code>SourceConnector</code> and add a couple of fields that will store parsed configuration information (the filename to read from and the topic to send data to):</p>

    <pre class="brush: java;">
    public class FileStreamSourceConnector extends SourceConnector {
        private String filename;
        private String topic;
    </pre>

    <p>The easiest method to fill in is <code>taskClass()</code>, which defines the class that should be instantiated in worker processes to actually read the data:</p>

    <pre class="brush: java;">
    @Override
    public Class&lt;? extends Task&gt; taskClass() {
        return FileStreamSourceTask.class;
    }
    </pre>

    <p>We will define the <code>FileStreamSourceTask</code> class below. Next, we add some standard lifecycle methods, <code>start()</code> and <code>stop()</code></p>:

    <pre class="brush: java;">
    @Override
    public void start(Map&lt;String, String&gt; props) {
        // The complete version includes error handling as well.
        filename = props.get(FILE_CONFIG);
        topic = props.get(TOPIC_CONFIG);
    }

    @Override
    public void stop() {
        // Nothing to do since no background monitoring is required.
    }
    </pre>

    <p>Finally, the real core of the implementation is in <code>taskConfigs()</code>. In this case we are only
        handling a single file, so even though we may be permitted to generate more tasks as per the
        <code>maxTasks</code> argument, we return a list with only one entry:</p>

    <pre class="brush: java;">
    @Override
    public List&lt;Map&lt;String, String&gt;&gt; taskConfigs(int maxTasks) {
        ArrayList&lt;Map&lt;String, String&gt;&gt; configs = new ArrayList&lt;&gt;();
        // Only one input stream makes sense.
        Map&lt;String, String&gt; config = new HashMap&lt;&gt;();
        if (filename != null)
            config.put(FILE_CONFIG, filename);
        config.put(TOPIC_CONFIG, topic);
        configs.add(config);
        return configs;
    }
    </pre>

    <p>Although not used in the example, <code>SourceTask</code> also provides two APIs to commit offsets in the source system: <code>commit</code> and <code>commitRecord</code>. The APIs are provided for source systems which have an acknowledgement mechanism for messages. Overriding these methods allows the source connector to acknowledge messages in the source system, either in bulk or individually, once they have been written to Kafka.
        The <code>commit</code> API stores the offsets in the source system, up to the offsets that have been returned by <code>poll</code>. The implementation of this API should block until the commit is complete. The <code>commitRecord</code> API saves the offset in the source system for each <code>SourceRecord</code> after it is written to Kafka. As Kafka Connect will record offsets automatically, <code>SourceTask</code>s are not required to implement them. In cases where a connector does need to acknowledge messages in the source system, only one of the APIs is typically required.</p>

    <p>Even with multiple tasks, this method implementation is usually pretty simple. It just has to determine the number of input tasks, which may require contacting the remote service it is pulling data from, and then divvy them up. Because some patterns for splitting work among tasks are so common, some utilities are provided in <code>ConnectorUtils</code> to simplify these cases.</p>

    <p>Note that this simple example does not include dynamic input. See the discussion in the next section for how to trigger updates to task configs.</p>

    <h5><a id="connect_taskexample" href="#connect_taskexample">Task Example - Source Task</a></h5>

    <p>接下来我们将描述相应的 <code>SourceTask</code> 的实现。 这里的实现很简短, 但如果要想完全涵盖本指南的内容就太长了。我们将用伪代码描述大多数实现，你也可以参考完整示例的源代码。</p>

    <p>就像 Connector 一样，我们需要创建一个类并继承对应的基类<code>Task</code>。 它也有一些标准的生命周期方法：</p>


    <pre class="brush: java;">
    public class FileStreamSourceTask extends SourceTask {
        String filename;
        InputStream stream;
        String topic;

        @Override
        public void start(Map&lt;String, String&gt; props) {
            filename = props.get(FileStreamSourceConnector.FILE_CONFIG);
            stream = openOrThrowError(filename);
            topic = props.get(FileStreamSourceConnector.TOPIC_CONFIG);
        }

        @Override
        public synchronized void stop() {
            stream.close();
        }
    </pre>

    <p>这是一个轻量级的简化版本，但表明这些方法应该相对简单，并且它们唯一要做的就是分配和释放资源。 关于这个实现有两点要注意： 第一， <code>start()</code> 方法没有处理从之前的 offset 恢复的情形，这一点将在后面的章节中讨论。第二， <code>stop()</code> 方法是同步的。这是必须的，因为 <code>SourceTasks</code> 有一个专用的线程，可以无限期的阻塞下去，所以它们需要通过 Worker 中另一个线程的调用来停止。</p>

    <p>接下来，我们要实现 task 的主要功能，<code>poll()</code>方法负责从输入系统获取事件并返回一个<code>List&lt;SourceRecord&gt;</code>：</p>

    <pre class="brush: java;">
    @Override
    public List&lt;SourceRecord&gt; poll() throws InterruptedException {
        try {
            ArrayList&lt;SourceRecord&gt; records = new ArrayList&lt;&gt;();
            while (streamValid(stream) &amp;&amp; records.isEmpty()) {
                LineAndOffset line = readToNextLine(stream);
                if (line != null) {
                    Map&lt;String, Object&gt; sourcePartition = Collections.singletonMap("filename", filename);
                    Map&lt;String, Object&gt; sourceOffset = Collections.singletonMap("position", streamOffset);
                    records.add(new SourceRecord(sourcePartition, sourceOffset, topic, Schema.STRING_SCHEMA, line));
                } else {
                    Thread.sleep(1);
                }
            }
            return records;
        } catch (IOException e) {
            // Underlying stream was killed, probably as a result of calling stop. Allow to return
            // null, and driving thread will handle any shutdown if necessary.
        }
        return null;
    }
    </pre>

    <p>同样，我们也省略了一些细节，但是我们可以看到重要的步骤：<code>poll()</code>方法会被反复调用，并且对于每次调用，它会循环尝试从文件中读取记录。对于它读取的每一行，它也会跟踪文件偏移量。它使用这些信息创建一个输出 <code>SourceRecord</code>，带有四部分信息：source partition（本示例中只有一个，即正在读取的单个文件），source offset（文件中的字节偏移量），output topic name 和 output value（读取的行，并且我们使用一个模式指定该值始终是一个字符串）。 SourceRecord构造函数的其他变体还可以包含特定的输出分区和密钥。</p>

    <p>.注意，这个实现使用了普通的Java <code>InputStream</code>接口，没有可用数据时会休眠。这是可以接受的，因为 Kafka 为每个 task 提供了一个专用线程。虽然 task 的实现必须覆盖基本的 <code>poll()</code> 接口，但实现起来有很大的灵活性。在这种情况下，基于 NIO 的实现将更加高效，但这种简单的方法很有效，实现起来很快，并且与旧版本的 Java 兼容。</p>

    <h5><a id="connect_sinktasks" href="#connect_sinktasks">Sink Tasks</a></h5>

    <p>之前的部分描述了如何实现一个简单的<code>SourceTask</code>。 与 <code>SourceConnector</code> 和 <code>SinkConnector</code>不同的是，<code>SourceTask</code> 和 <code>SinkTask</code> 有着完全不同的接口：<code>SourceTask</code> 使用 pull 接口， <code>SinkTask</code> 使用 push 接口。两者共享相同的生命周期方法，但 <code>SinkTask</code> 接口有点特殊：</p>

    <pre class="brush: java;">
    public abstract class SinkTask implements Task {
        public void initialize(SinkTaskContext context) {
            this.context = context;
        }

        public abstract void put(Collection&lt;SinkRecord&gt; records);

        public void flush(Map&lt;TopicPartition, OffsetAndMetadata&gt; currentOffsets) {
        }
    </pre>

    <p><code>SinkTask</code> 的文档包含了所有的细节，但这个接口几乎和 <code>SourceTask</code>一样简单。<code>put()</code>方法应包含了大部分实现，包括接受 <code>SinkRecords</code> 集合，以及执行任何必需的转换，并将它们存储在目标系统中。此方法无需确保数据在返回之前已完全写入目标系统。实际上，在许多情况下 internal buffering 会很有用，使得我们可以一次发送整批记录，从而减少将事件插入下游数据存储的开销。<code>SinkRecords</code> 而本质上和 <code>SourceRecords</code> 有着相同的信息: Kafka topic, partition, offset and the event key and value。</p>

    <p><code>flush()</code> 方法用于提交 offset 数据，这使得 task 可以从故障中恢复并且从安全点恢复，因此不会有事件丢失。该方法应该将任何未完成的数据推送到目标系统，然后阻塞，直到写入被确认。 参数 <code>offsets</code> 通常会被忽略，但在某些情况下很有用，例如实现需要在目标存储区中存储 offset 信息以提供精确的 exactly-once 交付。例如，HDFS connector 可以做到这一点，它使用原子性移动操作来确保 <code>flush()</code> 操作以原子方式将数据和 offset 提交到 HDFS 中的最终位置。</p>


    <h5><a id="connect_resuming" href="#connect_resuming">Resuming from Previous Offsets</a></h5>

    <p><code>SourceTask</code> 的实现包含每条记录的 stream ID (本示例中是输入的文件名) 和 offset (记录在文件中的位置)。该框架使用它定期提交 offset 数据，以便在发生故障的情况下，task 可以恢复并且最小化 reprocess 和 可能重复的事件的数量（或者从最近 Kafka Connect 正常停止过的 offset 恢复，例如在独立模式下或作业重新配置导致的停止）。这个提交过程由框架完全自动化，但只有 connector 知道如何退回到 input stream 中的正确位置并从该位置恢复。</p>

    <p>为了在启动时正确恢复，task 可以使用它的 <code>initialize()</code> 方法中传入的 <code>SourceContext</code> 来访问 offset 数据。在 <code>initialize()</code> 中，我们会添加更多的代码来读取 offset 数据（如果存在）并寻找对应的位置：</p>

    <pre class="brush: java;">
        stream = new FileInputStream(filename);
        Map&lt;String, Object&gt; offset = context.offsetStorageReader().offset(Collections.singletonMap(FILENAME_FIELD, filename));
        if (offset != null) {
            Long lastRecordedOffset = (Long) offset.get("position");
            if (lastRecordedOffset != null)
                seekToOffset(stream, lastRecordedOffset);
        }
    </pre>

    <p>当然，您可能需要为每个 input stream 读取多个 key。<code>OffsetStorageReader</code> 接口允许您发出批量读取以有效加载所有 offset，然后通过查找每个输入流来把它们放到对应的位置。</p>

    <h4><a id="connect_dynamicio" href="#connect_dynamicio">Dynamic Input/Output Streams</a></h4>

    <p>Kafka Connect is intended to define bulk data copying jobs, such as copying an entire database rather than creating many jobs to copy each table individually. One consequence of this design is that the set of input or output streams for a connector can vary over time.</p>

    <p>Source connectors need to monitor the source system for changes, e.g. table additions/deletions in a database. When they pick up changes, they should notify the framework via the <code>ConnectorContext</code> object that reconfiguration is necessary. For example, in a <code>SourceConnector</code>:</p>

    <pre class="brush: java;">
        if (inputsChanged())
            this.context.requestTaskReconfiguration();
    </pre>

    <p>The framework will promptly request new configuration information and update the tasks, allowing them to gracefully commit their progress before reconfiguring them. Note that in the <code>SourceConnector</code> this monitoring is currently left up to the connector implementation. If an extra thread is required to perform this monitoring, the connector must allocate it itself.</p>

    <p>Ideally this code for monitoring changes would be isolated to the <code>Connector</code> and tasks would not need to worry about them. However, changes can also affect tasks, most commonly when one of their input streams is destroyed in the input system, e.g. if a table is dropped from a database. If the <code>Task</code> encounters the issue before the <code>Connector</code>, which will be common if the <code>Connector</code> needs to poll for changes, the <code>Task</code> will need to handle the subsequent error. Thankfully, this can usually be handled simply by catching and handling the appropriate exception.</p>

    <p><code>SinkConnectors</code> usually only have to handle the addition of streams, which may translate to new entries in their outputs (e.g., a new database table). The framework manages any changes to the Kafka input, such as when the set of input topics changes because of a regex subscription. <code>SinkTasks</code> should expect new input streams, which may require creating new resources in the downstream system, such as a new table in a database. The trickiest situation to handle in these cases may be conflicts between multiple <code>SinkTasks</code> seeing a new input stream for the first time and simultaneously trying to create the new resource. <code>SinkConnectors</code>, on the other hand, will generally require no special code for handling a dynamic set of streams.</p>

    <h4><a id="connect_configs" href="#connect_configs">Connect Configuration Validation</a></h4>

    <p>Kafka Connect allows you to validate connector configurations before submitting a connector to be executed and can provide feedback about errors and recommended values. To take advantage of this, connector developers need to provide an implementation of <code>config()</code> to expose the configuration definition to the framework.</p>

    <p>The following code in <code>FileStreamSourceConnector</code> defines the configuration and exposes it to the framework.</p>

    <pre class="brush: java;">
        private static final ConfigDef CONFIG_DEF = new ConfigDef()
            .define(FILE_CONFIG, Type.STRING, Importance.HIGH, "Source filename.")
            .define(TOPIC_CONFIG, Type.STRING, Importance.HIGH, "The topic to publish data to");

        public ConfigDef config() {
            return CONFIG_DEF;
        }
    </pre>

    <p><code>ConfigDef</code> class is used for specifying the set of expected configurations. For each configuration, you can specify the name, the type, the default value, the documentation, the group information, the order in the group, the width of the configuration value and the name suitable for display in the UI. Plus, you can provide special validation logic used for single configuration validation by overriding the <code>Validator</code> class. Moreover, as there may be dependencies between configurations, for example, the valid values and visibility of a configuration may change according to the values of other configurations. To handle this, <code>ConfigDef</code> allows you to specify the dependents of a configuration and to provide an implementation of <code>Recommender</code> to get valid values and set visibility of a configuration given the current configuration values.</p>

    <p>Also, the <code>validate()</code> method in <code>Connector</code> provides a default validation implementation which returns a list of allowed configurations together with configuration errors and recommended values for each configuration. However, it does not use the recommended values for configuration validation. You may provide an override of the default implementation for customized configuration validation, which may use the recommended values.</p>

    <h4><a id="connect_schemas" href="#connect_schemas">Working with Schemas</a></h4>

    <p>The FileStream connectors are good examples because they are simple, but they also have trivially structured data -- each line is just a string. Almost all practical connectors will need schemas with more complex data formats.</p>

    <p>To create more complex data, you'll need to work with the Kafka Connect <code>data</code> API. Most structured records will need to interact with two classes in addition to primitive types: <code>Schema</code> and <code>Struct</code>.</p>

    <p>The API documentation provides a complete reference, but here is a simple example creating a <code>Schema</code> and <code>Struct</code>:</p>

    <pre class="brush: java;">
    Schema schema = SchemaBuilder.struct().name(NAME)
        .field("name", Schema.STRING_SCHEMA)
        .field("age", Schema.INT_SCHEMA)
        .field("admin", new SchemaBuilder.boolean().defaultValue(false).build())
        .build();

    Struct struct = new Struct(schema)
        .put("name", "Barbara Liskov")
        .put("age", 75);
    </pre>

    <p>If you are implementing a source connector, you'll need to decide when and how to create schemas. Where possible, you should avoid recomputing them as much as possible. For example, if your connector is guaranteed to have a fixed schema, create it statically and reuse a single instance.</p>

    <p>However, many connectors will have dynamic schemas. One simple example of this is a database connector. Considering even just a single table, the schema will not be predefined for the entire connector (as it varies from table to table). But it also may not be fixed for a single table over the lifetime of the connector since the user may execute an <code>ALTER TABLE</code> command. The connector must be able to detect these changes and react appropriately.</p>

    <p>Sink connectors are usually simpler because they are consuming data and therefore do not need to create schemas. However, they should take just as much care to validate that the schemas they receive have the expected format. When the schema does not match -- usually indicating the upstream producer is generating invalid data that cannot be correctly translated to the destination system -- sink connectors should throw an exception to indicate this error to the system.</p>

    <h4><a id="connect_administration" href="#connect_administration">Kafka Connect Administration</a></h4>

    <p>
        Kafka Connect's <a href="#connect_rest">REST layer</a> provides a set of APIs to enable administration of the cluster. This includes APIs to view the configuration of connectors and the status of their tasks, as well as to alter their current behavior (e.g. changing configuration and restarting tasks).
    </p>

    <p>
        When a connector is first submitted to the cluster, the workers rebalance the full set of connectors in the cluster and their tasks so that each worker has approximately the same amount of work. This same rebalancing procedure is also used when connectors increase or decrease the number of tasks they require, or when a connector's configuration is changed. You can use the REST API to view the current status of a connector and its tasks, including the id of the worker to which each was assigned. For example, querying the status of a file source (using <code>GET /connectors/file-source/status</code>) might produce output like the following:
    </p>

    <pre class="brush: json;">
    {
    "name": "file-source",
    "connector": {
        "state": "RUNNING",
        "worker_id": "192.168.1.208:8083"
    },
    "tasks": [
        {
        "id": 0,
        "state": "RUNNING",
        "worker_id": "192.168.1.209:8083"
        }
    ]
    }
    </pre>

    <p>
        Connectors and their tasks publish status updates to a shared topic (configured with <code>status.storage.topic</code>) which all workers in the cluster monitor. Because the workers consume this topic asynchronously, there is typically a (short) delay before a state change is visible through the status API. The following states are possible for a connector or one of its tasks:
    </p>

    <ul>
        <li><b>UNASSIGNED:</b> The connector/task has not yet been assigned to a worker.</li>
        <li><b>RUNNING:</b> The connector/task is running.</li>
        <li><b>PAUSED:</b> The connector/task has been administratively paused.</li>
        <li><b>FAILED:</b> The connector/task has failed (usually by raising an exception, which is reported in the status output).</li>
    </ul>

    <p>
        In most cases, connector and task states will match, though they may be different for short periods of time when changes are occurring or if tasks have failed. For example, when a connector is first started, there may be a noticeable delay before the connector and its tasks have all transitioned to the RUNNING state. States will also diverge when tasks fail since Connect does not automatically restart failed tasks. To restart a connector/task manually, you can use the restart APIs listed above. Note that if you try to restart a task while a rebalance is taking place, Connect will return a 409 (Conflict) status code. You can retry after the rebalance completes, but it might not be necessary since rebalances effectively restart all the connectors and tasks in the cluster.
    </p>

    <p>
        It's sometimes useful to temporarily stop the message processing of a connector. For example, if the remote system is undergoing maintenance, it would be preferable for source connectors to stop polling it for new data instead of filling logs with exception spam. For this use case, Connect offers a pause/resume API. While a source connector is paused, Connect will stop polling it for additional records. While a sink connector is paused, Connect will stop pushing new messages to it. The pause state is persistent, so even if you restart the cluster, the connector will not begin message processing again until the task has been resumed. Note that there may be a delay before all of a connector's tasks have transitioned to the PAUSED state since it may take time for them to finish whatever processing they were in the middle of when being paused. Additionally, failed tasks will not transition to the PAUSED state until they have been restarted.
    </p>
</script>

<div class="p-connect"></div>
